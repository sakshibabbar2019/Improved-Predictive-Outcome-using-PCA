{"cells":[{"cell_type":"markdown","metadata":{"id":"S2XJqOAKOfxm"},"source":["# Using PCA to Improve Classification Results\n"]},{"cell_type":"markdown","metadata":{"id":"CqvIfi8QOfxx"},"source":["# 1. Loading Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3bn_c_3HOfxy"},"outputs":[],"source":["# pandas and numpy for data manipulation\n","import numpy as np\n","import pandas as pd\n","# importing matplotlib to support visualization\n","import matplotlib.pyplot as plt\n","\n","# importing PCA from sklearn\n","from sklearn.decomposition import PCA\n","\n","# importing train_test_split to create HOld-Out enviornment\n","from sklearn.model_selection import train_test_split\n","# imports Decision tree classifier\n","from sklearn import tree \n","# imports sklearn builtin metrices\n","from sklearn import metrics\n","# imports preprocessing functions from sklearn\n","from sklearn import preprocessing\n"]},{"cell_type":"markdown","metadata":{"id":"wiQY3HtJOfx1"},"source":["# 2. Loading Data set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NJkz2aL3Ofx1","outputId":"4dc9651d-f9a4-4e1a-a15f-dcf45a72e4d1"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 303 entries, 0 to 302\n","Data columns (total 14 columns):\n","age         303 non-null int64\n","sex         303 non-null int64\n","cp          303 non-null int64\n","trestbps    303 non-null int64\n","chol        303 non-null int64\n","fbs         303 non-null int64\n","restecg     303 non-null int64\n","thalach     303 non-null int64\n","exang       303 non-null int64\n","oldpeak     303 non-null float64\n","slope       303 non-null int64\n","ca          303 non-null int64\n","thal        303 non-null int64\n","target      303 non-null int64\n","dtypes: float64(1), int64(13)\n","memory usage: 33.2 KB\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>sex</th>\n","      <th>cp</th>\n","      <th>trestbps</th>\n","      <th>chol</th>\n","      <th>fbs</th>\n","      <th>restecg</th>\n","      <th>thalach</th>\n","      <th>exang</th>\n","      <th>oldpeak</th>\n","      <th>slope</th>\n","      <th>ca</th>\n","      <th>thal</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>303.000000</td>\n","      <td>303.000000</td>\n","      <td>303.000000</td>\n","      <td>303.000000</td>\n","      <td>303.000000</td>\n","      <td>303.000000</td>\n","      <td>303.000000</td>\n","      <td>303.000000</td>\n","      <td>303.000000</td>\n","      <td>303.000000</td>\n","      <td>303.000000</td>\n","      <td>303.000000</td>\n","      <td>303.000000</td>\n","      <td>303.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>54.366337</td>\n","      <td>0.683168</td>\n","      <td>0.966997</td>\n","      <td>131.623762</td>\n","      <td>246.264026</td>\n","      <td>0.148515</td>\n","      <td>0.528053</td>\n","      <td>149.646865</td>\n","      <td>0.326733</td>\n","      <td>1.039604</td>\n","      <td>1.399340</td>\n","      <td>0.729373</td>\n","      <td>2.313531</td>\n","      <td>0.544554</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>9.082101</td>\n","      <td>0.466011</td>\n","      <td>1.032052</td>\n","      <td>17.538143</td>\n","      <td>51.830751</td>\n","      <td>0.356198</td>\n","      <td>0.525860</td>\n","      <td>22.905161</td>\n","      <td>0.469794</td>\n","      <td>1.161075</td>\n","      <td>0.616226</td>\n","      <td>1.022606</td>\n","      <td>0.612277</td>\n","      <td>0.498835</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>29.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>94.000000</td>\n","      <td>126.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>71.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>47.500000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>120.000000</td>\n","      <td>211.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>133.500000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>2.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>55.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>130.000000</td>\n","      <td>240.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>153.000000</td>\n","      <td>0.000000</td>\n","      <td>0.800000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>2.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>61.000000</td>\n","      <td>1.000000</td>\n","      <td>2.000000</td>\n","      <td>140.000000</td>\n","      <td>274.500000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>166.000000</td>\n","      <td>1.000000</td>\n","      <td>1.600000</td>\n","      <td>2.000000</td>\n","      <td>1.000000</td>\n","      <td>3.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>77.000000</td>\n","      <td>1.000000</td>\n","      <td>3.000000</td>\n","      <td>200.000000</td>\n","      <td>564.000000</td>\n","      <td>1.000000</td>\n","      <td>2.000000</td>\n","      <td>202.000000</td>\n","      <td>1.000000</td>\n","      <td>6.200000</td>\n","      <td>2.000000</td>\n","      <td>4.000000</td>\n","      <td>3.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              age         sex          cp    trestbps        chol         fbs  \\\n","count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \n","mean    54.366337    0.683168    0.966997  131.623762  246.264026    0.148515   \n","std      9.082101    0.466011    1.032052   17.538143   51.830751    0.356198   \n","min     29.000000    0.000000    0.000000   94.000000  126.000000    0.000000   \n","25%     47.500000    0.000000    0.000000  120.000000  211.000000    0.000000   \n","50%     55.000000    1.000000    1.000000  130.000000  240.000000    0.000000   \n","75%     61.000000    1.000000    2.000000  140.000000  274.500000    0.000000   \n","max     77.000000    1.000000    3.000000  200.000000  564.000000    1.000000   \n","\n","          restecg     thalach       exang     oldpeak       slope          ca  \\\n","count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \n","mean     0.528053  149.646865    0.326733    1.039604    1.399340    0.729373   \n","std      0.525860   22.905161    0.469794    1.161075    0.616226    1.022606   \n","min      0.000000   71.000000    0.000000    0.000000    0.000000    0.000000   \n","25%      0.000000  133.500000    0.000000    0.000000    1.000000    0.000000   \n","50%      1.000000  153.000000    0.000000    0.800000    1.000000    0.000000   \n","75%      1.000000  166.000000    1.000000    1.600000    2.000000    1.000000   \n","max      2.000000  202.000000    1.000000    6.200000    2.000000    4.000000   \n","\n","             thal      target  \n","count  303.000000  303.000000  \n","mean     2.313531    0.544554  \n","std      0.612277    0.498835  \n","min      0.000000    0.000000  \n","25%      2.000000    0.000000  \n","50%      2.000000    1.000000  \n","75%      3.000000    1.000000  \n","max      3.000000    1.000000  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# reading data set\n","dataset = pd.read_csv(\"Data sets/heart.csv\")\n","dataset.info()\n","dataset.describe()\n"]},{"cell_type":"markdown","metadata":{"id":"YDrLWNT8Ofx3"},"source":["Data set has 303 rows and 14 features. The feature target is a class variable. The statistical properties of feature values differ a lot. Hence, it is important to first standardize the data set. "]},{"cell_type":"markdown","metadata":{"id":"zum5jXHaOfx4"},"source":["# 3. Standardize Data set &  Creating Train and Test sets under Hold-Out Method"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b93-NXVLOfx5","outputId":"4717321b-592a-4d39-bd24-c861b90ebbfb"},"outputs":[{"name":"stdout","output_type":"stream","text":["The sample training data without target feature\n","\n","           0         1         2         3         4         5         6   \\\n","134 -1.474158 -1.468418  0.032031 -0.321189  1.154427 -0.417635  0.898962   \n","143  1.393352 -1.468418 -0.938515 -1.463447 -0.449589 -0.417635  0.898962   \n","253  1.393352  0.681005 -0.938515 -1.806125  1.019148 -0.417635 -1.005832   \n","205 -0.260980  0.681005 -0.938515 -0.206964  0.168827 -0.417635  0.898962   \n","266  0.069886 -1.468418 -0.938515  2.762907  1.560262 -0.417635  2.803756   \n","\n","           7         8         9         10        11        12  \n","134  0.583939 -0.696631 -0.896862  0.976352 -0.714429 -0.512922  \n","143 -0.334401 -0.696631 -0.638053  0.976352  1.244593 -0.512922  \n","253 -1.077820  1.435481 -0.120436 -0.649113  1.244593 -0.512922  \n","205  0.496478  1.435481 -0.896862  0.976352  0.265082  1.123029  \n","266 -1.427664  1.435481  2.036303 -0.649113 -0.714429 -0.512922  \n","\n","The sample with only target feature\n","\n","134    1\n","143    1\n","253    0\n","205    0\n","266    0\n","Name: target, dtype: int64\n"]}],"source":["# My_data contains all data points from My_data set from from \n","# first feature to 12th feature(indicator features)\n","My_data = dataset.iloc[:,0:13] \n","\n","# Standardizing data set using preprocessing library in sklearn\n","Standardize_dataset= preprocessing.StandardScaler()\n","My_dataset = Standardize_dataset.fit_transform(My_data)\n","My_dataset=pd.DataFrame(My_dataset)\n","\n","\n","# My_target contains class information which is 13th feature in the\n","#data set\n","\n","My_data_target=dataset.iloc[:,13]\n","\n","# creating train and test data sets\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(My_dataset, My_data_target, test_size=0.8, random_state=10)\n","\n","print(\"The sample training data without target feature\\n\")\n","print(X_train.head())\n","print(\"\\nThe sample with only target feature\\n\")\n","print(Y_train.head())"]},{"cell_type":"markdown","metadata":{"id":"mQSv_4DcOfx6"},"source":["The feature values are standardized.  "]},{"cell_type":"markdown","metadata":{"id":"ZO5SstXcOfx7"},"source":["# 4. Building Decision Tree Classifier using Train Set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VBHw5p5mOfx8"},"outputs":[],"source":["#Create a Decision tree Classifier\n","\n","DTmodel_1 = tree.DecisionTreeClassifier() \n","\n","#Train the model using the training sets\n","\n","DTfitted_1 = DTmodel_1.fit(X_train, Y_train)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ryF2ZTu8Ofx8"},"source":["# 5. Testing Model Performance on Test Set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"avPJr2QzOfx9"},"outputs":[],"source":["\n","#Predict the response on the test data set\n","DT_predictions_1 = DTfitted_1.predict((X_test))\n"]},{"cell_type":"markdown","metadata":{"id":"PMxAwcSJOfx9"},"source":["# 6. Evaluating Model Performance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bX8mb3OmOfx-","outputId":"9ff7ddf1-51c2-42e7-d2f7-8375562cd921"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 69.0 %\n","---------------\n","Confusion matrix\n","---------------\n","[[72 43]\n"," [33 95]]\n","---------------\n","Classification report               precision    recall  f1-score   support\n","\n","     class 0       0.69      0.63      0.65       115\n","     class 1       0.69      0.74      0.71       128\n","\n","    accuracy                           0.69       243\n","   macro avg       0.69      0.68      0.68       243\n","weighted avg       0.69      0.69      0.69       243\n","\n"]}],"source":["# Computing Model Accuracy\n","\n","print(\"Accuracy:\",round(metrics.accuracy_score(Y_test, DT_predictions_1),2) * 100, \"%\")\n","\n","print (\"---------------\")\n","\n","# Printing confusion matrix\n","\n","print (\"Confusion matrix\")\n","\n","print (\"---------------\")\n","\n","print(metrics.confusion_matrix(Y_test, DT_predictions_1))\n","\n","# Model detailed classification report\n","target_names = ['class 0', 'class 1']\n","\n","\n","print (\"---------------\")\n","\n","print(\"Classification report\", metrics.classification_report(Y_test, DT_predictions_1,target_names =target_names))"]},{"cell_type":"markdown","metadata":{"id":"OCyZT-7oOfx-"},"source":["Accuracy of the model has turned out to be 63%. It is a weak performance. In the next step, we will first transform data set using PCA and the classfier model will be retrained on the data set transformed by PCA. "]},{"cell_type":"markdown","metadata":{"id":"JRtJQ33kOfx_"},"source":["# 7. Applying PCA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g1XOVxIBOfx_"},"outputs":[],"source":["# Applying PCA with n_components set to .80. It means that we want \n","#PCA to perform dimensionality reduction such that 80% variance of\n","# the original data set is preserved. \n","pca = PCA(n_components=.80) \n","# fitting data set\n","pca.fit(My_dataset)\n","# transforming data set using new PC's discovered by PCA\n","data_new = pca.transform(My_dataset)\n","Data_new=pd.DataFrame(data_new)\n"]},{"cell_type":"markdown","metadata":{"id":"fkTUfNEeOfx_"},"source":["# 8. Printing Variance Explained by each PC "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QivXcacyOfyA","outputId":"8a18cce5-e9c2-4c64-88b7-4230e2575137"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of PC's discovered by PCA: 9\n",".............................\n","Total variance explained by 9 Pc's is  85.03 % \n"," \n",".............................\n","Printing discovered PC's \n","\n","           0         1         2         3         4         5         6  \\\n","0   0.314203  0.406149 -0.094077 -0.020662 -0.307153 -0.128296 -0.223730   \n","1   0.090838 -0.377792  0.554849 -0.255309  0.050704  0.054969 -0.162507   \n","2  -0.274607  0.297266  0.356974  0.287900  0.163179 -0.193411 -0.215390   \n","3   0.183920  0.438187  0.203849  0.022601  0.188138 -0.179460  0.332763   \n","4   0.117375  0.364514 -0.407825 -0.343410  0.320067 -0.104730  0.049329   \n","5   0.073640  0.317433  0.481736 -0.068605 -0.233442  0.249614  0.510818   \n","6  -0.127728 -0.220882 -0.089191  0.266096 -0.393667 -0.666813  0.396896   \n","7  -0.416498  0.077876  0.158255 -0.184125  0.323284 -0.120984  0.101473   \n","8   0.361267 -0.263118 -0.126356 -0.115056  0.034536  0.230699  0.449919   \n","9   0.419639 -0.052255  0.110343  0.326296  0.250579 -0.170080 -0.112888   \n","10 -0.379772  0.048374 -0.073818 -0.494849 -0.246823 -0.064069  0.055038   \n","11  0.273262  0.094147  0.183569 -0.328016 -0.435365 -0.182107 -0.337606   \n","12  0.222024 -0.200720  0.125011 -0.389191  0.331950 -0.508857  0.055165   \n","\n","           7         8  \n","0  -0.262477 -0.379000  \n","1  -0.175992 -0.198925  \n","2   0.047950 -0.351432  \n","3  -0.595334  0.350392  \n","4   0.372381 -0.153975  \n","5   0.432863 -0.177004  \n","6   0.099841 -0.038304  \n","7   0.143461  0.372044  \n","8  -0.112607 -0.058500  \n","9   0.192323  0.233603  \n","10 -0.261807 -0.028505  \n","11  0.259678  0.485808  \n","12  0.034349 -0.284201  \n",".............................\n","Printing size of components \n","\n","(13, 9)\n"]}],"source":["# It will print number of PC's discovered by PCA\n","print(\"Number of PC's discovered by PCA:\",pca.components_.shape[0] )\n","print(\".............................\")\n","# It will print total variance explained by all PC's\n","print(\"Total variance explained by %d Pc's is  %2.2f %% \\n \"%(pca.components_.shape[0]\n","     ,np.sum(pca.explained_variance_ratio_)*100))\n","print(\".............................\")\n","print(\"Printing discovered PC's \\n\")\n","All_PCs = pd.DataFrame(pca.components_)\n","print(All_PCs.T)\n","print(\".............................\")\n","print(\"Printing size of components \\n\")\n","print(All_PCs.T.shape)"]},{"cell_type":"markdown","metadata":{"id":"BMEJqGbTOfyA"},"source":["PCA disovered 9 PC's that explained 85% of the original data set. The resulted components are of size 13 x 9. Where every PC of total 9 discovered is formed by linear combination of 13 feature present in the data set."]},{"cell_type":"markdown","metadata":{"id":"3Sa2RoeROfyA"},"source":["# 9. Creating Train and Test set on transformed Dataset by PCA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3PKrVCllOfyB","outputId":"9996cfc4-7ae2-406b-e822-8cc9ad91a298"},"outputs":[{"name":"stdout","output_type":"stream","text":["The sample training data without target feature\n","\n","            0         1         2         3         4         5         6  \\\n","134 -2.225653  0.273038 -1.668147 -0.090445  0.276002 -0.415002  0.768777   \n","143 -0.430092  0.163350 -1.620222 -0.293148 -2.576352 -0.511867 -0.906733   \n","253  2.032234 -0.567301 -1.136519 -0.996070 -0.947685  1.382028 -1.317668   \n","205 -0.039806 -1.447947 -0.409823 -1.789855 -0.278015 -0.690443  1.056453   \n","266  2.353844  0.740692 -2.010684  1.921939  0.163153 -1.950980  2.769245   \n","\n","            7         8  \n","134  0.707948  0.408110  \n","143  0.418220  0.179649  \n","253  0.779066 -0.755247  \n","205 -0.412408 -0.092749  \n","266 -0.777634  0.967585  \n","\n","The sample with only target feature\n","\n","134    1\n","143    1\n","253    0\n","205    0\n","266    0\n","Name: target, dtype: int64\n"]}],"source":["\n","X_train_1, X_test_1, Y_train_1, Y_test_1 = train_test_split(Data_new, My_data_target, test_size=0.8, random_state=10)\n","\n","print(\"The sample training data without target feature\\n\")\n","print(X_train_1.head())\n","print(\"\\nThe sample with only target feature\\n\")\n","print(Y_train_1.head())"]},{"cell_type":"markdown","metadata":{"id":"n2BEBpuJOfyB"},"source":["# 10. Creating Decision Tree classifier on the Train Set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qxPtsg1ZOfyB"},"outputs":[],"source":["#Create a Decision tree Classifier\n","\n","DTmodel_2 = tree.DecisionTreeClassifier() \n","\n","#Train the model using the training sets\n","\n","DTfitted_2 = DTmodel_2.fit(X_train_1, Y_train_1)"]},{"cell_type":"markdown","metadata":{"id":"sTNA4VMhOfyB"},"source":["# 11. Testing Classifier on Test set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5uI-SbX4OfyC"},"outputs":[],"source":["\n","#Predict the response on the test data set\n","\n","DT_predictions_2 = DTfitted_2.predict((X_test_1))"]},{"cell_type":"markdown","metadata":{"id":"YIVe27kwOfyC"},"source":["# 12. Evaluating Performance of Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZTxYf2YQOfyC","outputId":"ebf1bf81-87e3-4b02-c3f8-17a6ded3d6bb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 74.0 %\n","---------------\n","Confusion matrix\n","---------------\n","[[85 30]\n"," [33 95]]\n","---------------\n","Classification report               precision    recall  f1-score   support\n","\n","     class 0       0.72      0.74      0.73       115\n","     class 1       0.76      0.74      0.75       128\n","\n","    accuracy                           0.74       243\n","   macro avg       0.74      0.74      0.74       243\n","weighted avg       0.74      0.74      0.74       243\n","\n"]}],"source":["# Computing Model Accuracy\n","\n","print(\"Accuracy:\",round(metrics.accuracy_score(Y_test_1, DT_predictions_2),2) * 100, \"%\")\n","\n","print (\"---------------\")\n","\n","# Printing confusion matrix\n","\n","print (\"Confusion matrix\")\n","\n","print (\"---------------\")\n","\n","print(metrics.confusion_matrix(Y_test_1, DT_predictions_2))\n","\n","# Model detailed classification report\n","target_names = ['class 0', 'class 1']\n","\n","\n","print (\"---------------\")\n","\n","print(\"Classification report\", metrics.classification_report(Y_test_1, DT_predictions_2,target_names =target_names))"]},{"cell_type":"markdown","metadata":{"id":"-_hJRGbCOfyC"},"source":["As indicated by the results, the performance of the model has improved from 63% to 75% when PCA was used for transformation of data set "]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}